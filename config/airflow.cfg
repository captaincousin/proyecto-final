[core]
# The home folder for airflow, default is ~/airflow
airflow_home = /opt/airflow

# The folder where your airflow pipelines live, most likely a
# subfolder in a code repository
dags_folder = /opt/airflow/dags

# The folder where airflow should store its log files
base_log_folder = /opt/airflow/logs

# Airflow can store logs remotely in AWS S3 or Google Cloud Storage. Users
# must supply an Airflow connection id that provides access to the storage
# location.
remote_logging = False

# Logging level
logging_level = INFO

# Logging class
# Specify the class that will specify the logging configuration
# This class has to be on the python classpath
# logging_config_class =

# How many seconds before timing out a python file import while filling the DagBag
dagbag_import_timeout = 30

# How often should stats be printed to the logs
print_stats_interval = 30

# How many seconds should the DagFileProcessor take to heartbeat before being
# killed and restarted
dag_file_processor_timeout = 120

# The frequency at which the master scheduler should run
scheduler_heartbeat_sec = 5

# The AIP to run the Gunicorn webserver
# web_server_master_timeout = 120
# web_server_worker_timeout = 120
# web_server_master_interval = 1

# Secret key to save connection passwords in the db
fernet_key =

# The amount of parallelism as a setting to the executor. This defines
# the max number of task instances that should run simultaneously
# in this airflow installation
parallelism = 32

# The number of task instances allowed to run concurrently by the scheduler
dag_concurrency = 16

# Are DAGs paused by default at creation
dags_are_paused_at_creation = True

# When not using pools, tasks are run in the "default pool",
# whose size is guided by this configuration
non_pooled_task_slot_count = 128

# The maximum number of active DAG runs per DAG
max_active_runs_per_dag = 16

# The maximum number of active tasks per DAG
max_active_tasks_per_dag = 16

# The maximum amount of time to wait for a dag to become healthy
# at the DAG creation time
dag_default_view = tree

# How long to wait before timing out a python file import for the importer
import_timeout = 30

# Set this to True to have the Dags / Plugins in a subdag or subplugin folder
# to be importable
dagbag_import_timeout = 30

# How long before timing out a python file import for the DAGs / Plugins
import_timeout = 30

# Whether to load and parse the examples that ship with Airflow. It's good
# to set this to False in production environments in order to reduce
# unnecessary queries to the database.
load_examples = False

[cli]
# if you change the port number here make sure to update the value in
# airflow/www/Dockerfile
endpoint_url = http://localhost:8080

[scheduler]
# The number of seconds to wait before the scheduler runs a job again
job_heartbeat_sec = 5

[scheduler_health_check]
# Set this value to true to enable the Dag File Processor heartbeat
# This checks the Dag File Processor every minute to ensure that it is
# alive and functioning properly
scheduler_heartbeat = True

[scheduler_job_stats]
# Scheduler job statistics are stored in the database periodically by default.
# If this configuration is set to True, Airflow will not record scheduler job
# stats. If it is set to False, scheduler job stats will be recorded to the
# database.
record_db_heartbeat = True

[webserver]
# The base url of your website as airflow cannot guess what domain or
# cname you are using. This is used in automated emails that
# airflow sends to point links to the right web server
base_url = http://localhost:8080

# The ip specified when starting the web server
web_server_host = 0.0.0.0

# The port on which to run the web server
web_server_port = 8080

# Paths to the SSL certificate and key for the web server. When both are
# provided SSL will be enabled. This does not change the web server port.
# The ssl cert file must have a .crt extension and the key file must have a
# .key extension for the web server to recognize them as ssl files.
#web_server_ssl_cert =
#web_server_ssl_key =
#web_server_ssl_ciphers = 'ALL:!SSLv2:!EXP:!LOW:!aNULL:!eNULL:!IDEA:!3DES:!MD5:!SSLv3'

# Number of seconds the webserver waits before killing gunicorn master that doesn't respond
# Keep the value high to avoid gunicorn master to be killed while uploading dags
web_server_master_timeout = 120

# Number of seconds the gunicorn webserver waits before timing out on a worker
# Keep the value high to avoid long response time during dags upload or a long render of some views.
web_server_worker_timeout = 120

# How often should stats be printed to the logs
print_stats_interval = 30

[api]
# How to authenticate users of the API
auth_backend = airflow.api.auth.backend.default

[users]
# This section is to be used for user configuration
# If you simply add users here without giving them
# a role they will not be able to login. Roles
# can be added in the Roles section below.
# Users will belong to all of the roles you have
# assigned to them in this list.
# For more information on how to create users and assign
# roles refer to the airflow documentation.
# http://airflow.apache.org/docs/apache-airflow/stable/cli-and-env-variables-ref.html#users
user_creator = 
	[email]
	user_email = user@example.com


[smtp]
# If you want airflow to send emails on retries, failure, and you want to use
# the airflow.utils.email.send_email_smtp function, you have to configure an
# smtp server here
smtp_host = smtp.gmail.com
smtp_starttls = False
smtp_ssl = True
smtp_user = caperez@avla.com
smtp_password = Dificultad1@
smtp_port = 587
smtp_mail_from = caperez@avla.com

"""
[smtp]
# If you want airflow to send emails on retries, failure, and you want to use
# the airflow.utils.email.send_email_smtp function, you have to configure an
# smtp server here
smtp_host = smtp.gmail.com
smtp_starttls = True
smtp_ssl = False
# Example: smtp_user = airflow
smtp_user = analytics_google@avla.com
#luisernesto.200892@gmail.com
# Example: smtp_password = airflow
smtp_password = #include_stdio.h
#ajmowtcrapbwjtau
smtp_port = 25
smtp_mail_from = analytics_google.avla.com
#luisernesto.200892@gmail.com
smtp_timeout = 30
smtp_retry_limit = 5
"""
# This section is to setup what database backend airflow should use.
# Choices include mysql, postgres, sqlite3, oracle, and mssql.
# Sqlite3 is an embedded database, so you don't have to worry about
# setting up a separate database
[database]
# The connection string to the database backend.
# When not using sqlite, you must use a full connection string like this:
# fernet_key = your_fernet_key
sql_alchemy_conn = sqlite:////opt/airflow/air
